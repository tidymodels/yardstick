% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prob-average_precision.R
\name{average_precision}
\alias{average_precision}
\alias{average_precision.data.frame}
\alias{average_precision_vec}
\title{Area under the precision recall curve}
\usage{
average_precision(data, ...)

\method{average_precision}{data.frame}(
  data,
  truth,
  ...,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level()
)

average_precision_vec(
  truth,
  estimate,
  estimator = NULL,
  na_rm = TRUE,
  event_level = yardstick_event_level(),
  ...
)
}
\arguments{
\item{data}{A \code{data.frame} containing the \code{truth} and \code{estimate}
columns.}

\item{...}{A set of unquoted column names or one or more
\code{dplyr} selector functions to choose which variables contain the
class probabilities. If \code{truth} is binary, only 1 column should be selected.
Otherwise, there should be as many columns as factor levels of \code{truth}.}

\item{truth}{The column identifier for the true class results
(that is a \code{factor}). This should be an unquoted column name although
this argument is passed by expression and supports
\link[rlang:nse-force]{quasiquotation} (you can unquote column
names). For \verb{_vec()} functions, a \code{factor} vector.}

\item{estimator}{One of \code{"binary"}, \code{"macro"}, or \code{"macro_weighted"} to
specify the type of averaging to be done. \code{"binary"} is only relevant for
the two class case. The other two are general methods for calculating
multiclass metrics. The default will automatically choose \code{"binary"} or
\code{"macro"} based on \code{truth}.}

\item{na_rm}{A \code{logical} value indicating whether \code{NA}
values should be stripped before the computation proceeds.}

\item{event_level}{A single string. Either \code{"first"} or \code{"second"} to specify
which level of \code{truth} to consider as the "event". This argument is only
applicable when \code{estimator = "binary"}. The default uses an
internal helper that generally defaults to \code{"first"}, however, if the
deprecated global option \code{yardstick.event_first} is set, that will be
used instead with a warning.}

\item{estimate}{If \code{truth} is binary, a numeric vector of class probabilities
corresponding to the "relevant" class. Otherwise, a matrix with as many
columns as factor levels of \code{truth}. \emph{It is assumed that these are in the
same order as the levels of \code{truth}.}}
}
\value{
A \code{tibble} with columns \code{.metric}, \code{.estimator},
and \code{.estimate} and 1 row of values.

For grouped data frames, the number of rows returned will be the same as
the number of groups.

For \code{average_precision_vec()}, a single \code{numeric} value (or \code{NA}).
}
\description{
\code{average_precision()} is an alternative to \code{pr_auc()} that avoids any
ambiguity about what the value of \code{precision} should be when \code{recall == 0}
and there are not yet any false positive values (some say it should be \code{0},
others say \code{1}, others say undefined).

It computes a weighted average of the precision values returned from
\code{\link[=pr_curve]{pr_curve()}}, where the weights are the increase in recall from the previous
threshold. See \code{\link[=pr_curve]{pr_curve()}} for the full curve.
}
\details{
The computation for average precision is a weighted average of the precision
values. Assuming you have \code{n} rows returned from \code{\link[=pr_curve]{pr_curve()}}, it is a sum
from \code{2} to \code{n}, multiplying the precision value \code{p_i} by the increase in
recall over the previous threshold, \code{r_i - r_(i-1)}.

\deqn{AP = \sum (r_{i} - r_{i-1}) * p_i}

By summing from \code{2} to \code{n}, the precision value \code{p_1} is never used. While
\code{\link[=pr_curve]{pr_curve()}} returns a value for \code{p_1}, it is technically undefined as
\code{tp / (tp + fp)} with \code{tp = 0} and \code{fp = 0}. A common convention is to use
\code{1} for \code{p_1}, but this metric has the nice property of avoiding the
ambiguity. On the other hand, \code{r_1} is well defined as long as there are
some events (\code{p}), and it is \code{tp / p} with \code{tp = 0}, so \code{r_1 = 0}.

When \code{p_1} is defined as \code{1}, the \code{average_precision()} and \code{roc_auc()}
values are often very close to one another.
}
\section{Multiclass}{


Macro and macro-weighted averaging is available for this metric.
The default is to select macro averaging if a \code{truth} factor with more
than 2 levels is provided. Otherwise, a standard binary calculation is done.
See \code{vignette("multiclass", "yardstick")} for more information.
}

\section{Relevant Level}{


There is no common convention on which factor level should
automatically be considered the "event" or "positive" result
when computing binary classification metrics. In \code{yardstick}, the default
is to use the \emph{first} level. To alter this, change the argument
\code{event_level} to \code{"second"} to consider the \emph{last} level of the factor the
level of interest. For multiclass extensions involving one-vs-all
comparisons (such as macro averaging), this option is ignored and
the "one" level is always the relevant result.
}

\examples{
# ---------------------------------------------------------------------------
# Two class example

# `truth` is a 2 level factor. The first level is `"Class1"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(two_class_example)

# Binary metrics using class probabilities take a factor `truth` column,
# and a single class probability column containing the probabilities of
# the event of interest. Here, since `"Class1"` is the first level of
# `"truth"`, it is the event of interest and we pass in probabilities for it.
average_precision(two_class_example, truth, Class1)

# ---------------------------------------------------------------------------
# Multiclass example

# `obs` is a 4 level factor. The first level is `"VF"`, which is the
# "event of interest" by default in yardstick. See the Relevant Level
# section above.
data(hpc_cv)

# You can use the col1:colN tidyselect syntax
library(dplyr)
hpc_cv \%>\%
  filter(Resample == "Fold01") \%>\%
  average_precision(obs, VF:L)

# Change the first level of `obs` from `"VF"` to `"M"` to alter the
# event of interest. The class probability columns should be supplied
# in the same order as the levels.
hpc_cv \%>\%
  filter(Resample == "Fold01") \%>\%
  mutate(obs = relevel(obs, "M")) \%>\%
  average_precision(obs, M, VF:L)

# Groups are respected
hpc_cv \%>\%
  group_by(Resample) \%>\%
  average_precision(obs, VF:L)

# Weighted macro averaging
hpc_cv \%>\%
  group_by(Resample) \%>\%
  average_precision(obs, VF:L, estimator = "macro_weighted")

# Vector version
# Supply a matrix of class probabilities
fold1 <- hpc_cv \%>\%
  filter(Resample == "Fold01")

average_precision_vec(
   truth = fold1$obs,
   matrix(
     c(fold1$VF, fold1$F, fold1$M, fold1$L),
     ncol = 4
   )
)

}
\seealso{
\code{\link[=pr_curve]{pr_curve()}} for computing the full precision recall curve.

\code{\link[=pr_auc]{pr_auc()}} for computing the area under the precision recall curve using
the trapezoidal rule.

Other class probability metrics: 
\code{\link{gain_capture}()},
\code{\link{mn_log_loss}()},
\code{\link{pr_auc}()},
\code{\link{roc_auc}()},
\code{\link{roc_aunp}()},
\code{\link{roc_aunu}()}
}
\concept{class probability metrics}
